{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3506f092",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-08T00:35:26.286730Z",
     "iopub.status.busy": "2023-02-08T00:35:26.286091Z",
     "iopub.status.idle": "2023-02-08T00:35:26.302711Z",
     "shell.execute_reply": "2023-02-08T00:35:26.301538Z"
    },
    "papermill": {
     "duration": 0.029379,
     "end_time": "2023-02-08T00:35:26.305693",
     "exception": false,
     "start_time": "2023-02-08T00:35:26.276314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/digit-recognizer/sample_submission.csv\n",
      "/kaggle/input/digit-recognizer/train.csv\n",
      "/kaggle/input/digit-recognizer/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a54b935",
   "metadata": {
    "papermill": {
     "duration": 0.006456,
     "end_time": "2023-02-08T00:35:26.319719",
     "exception": false,
     "start_time": "2023-02-08T00:35:26.313263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Introduction**\n",
    "Welcome to my first notebook! I will be writing code along with my thought process and current knowledge upon the topics I will be diving into. Some basics of Machine Learning that I have learned through the wonders of YouTube are:\n",
    "* Weights and Biases\n",
    "* Forward and Back Propogation\n",
    "* Basics of Gradient Descent\n",
    "* Activation Functions & Softmax / Argmax Functions\n",
    "\n",
    "I'm not sure how the formatting of Jupyter Notebooks should be or if I'm following protocol with my informal language, however, I believe it would be fun to show off this project as a sort of journey I went down and to have logs and comments that people can follow along to and not have it just be long bricks of text explaining what what does and how does this work because, I won't exactly know how everything works at this point! However, my goal is to learn the deeper level of the concepts I hold surface level information on to come back to this and continue my path down Machine Learning.\n",
    "\n",
    "Hi! I'm **Aidan** and as of right now I am a Freshman in Stevens Institute of Technology, I HAVE made a previous submission regarding MNIST digits and Machine Learning and what not, but I have not made a formal Jupyter notebook so I will be trying to make one on an account that I will continue to use in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c842017",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T00:35:26.335078Z",
     "iopub.status.busy": "2023-02-08T00:35:26.334434Z",
     "iopub.status.idle": "2023-02-08T00:35:32.697521Z",
     "shell.execute_reply": "2023-02-08T00:35:32.696314Z"
    },
    "papermill": {
     "duration": 6.37391,
     "end_time": "2023-02-08T00:35:32.700164",
     "exception": false,
     "start_time": "2023-02-08T00:35:26.326254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve testing and training data\n",
    "test_data = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n",
    "train_data = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56001e59",
   "metadata": {
    "papermill": {
     "duration": 0.006465,
     "end_time": "2023-02-08T00:35:32.713369",
     "exception": false,
     "start_time": "2023-02-08T00:35:32.706904",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So the first thing we have to do obviously is to get the data from the CSV files provided. Nothing really new learned here, I knew how to work with **pandas** before working on this (pretty important pre-requisite to Python Machine Learning). \n",
    "\n",
    "Thankfully, the competition provided the data in two seperate CSV files for training and testing (how kind!) so there will be no splitting of numpy arrays or splitting data into training and testing data, because the work was done for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0559380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T00:35:32.728733Z",
     "iopub.status.busy": "2023-02-08T00:35:32.727813Z",
     "iopub.status.idle": "2023-02-08T00:35:33.623421Z",
     "shell.execute_reply": "2023-02-08T00:35:33.622206Z"
    },
    "papermill": {
     "duration": 0.905939,
     "end_time": "2023-02-08T00:35:33.625962",
     "exception": false,
     "start_time": "2023-02-08T00:35:32.720023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing data\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "test_data = test_data.T # transpose the data\n",
    "X_test = test_data\n",
    "X_test = X_test / 255\n",
    "\n",
    "# training data\n",
    "train_data = np.array(train_data)\n",
    "np.random.shuffle(train_data) # shuffle the data around\n",
    "\n",
    "train_data = train_data.T # transpose the data\n",
    "Y_train = train_data[0]\n",
    "X_train = train_data[1:]\n",
    "X_train = X_train / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a942a7",
   "metadata": {
    "papermill": {
     "duration": 0.006782,
     "end_time": "2023-02-08T00:35:33.639910",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.633128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Data Conversion**\n",
    "So here we convert the CSV data into numpy arrays so we can mold and mash the data however we want to.\n",
    "\n",
    "> *FUNNY STORY*: When I first did this competition, I shuffled the test data, and I was so confused as to how my predictions were so off when submitting them to the competition, and it took forever to find it... First submission was an accuracy of 5.25%\n",
    "\n",
    "The data is transposed (changing the rows into columns and vise versa) to make it easier to extract the Y values in the training data a.k.a. the expected output to the X data. The expected outputs are then stored in **Y_train** and the inputs are stored in **X_train**.\n",
    "\n",
    "I'm actually unsure as to why we divide the data by *255*, I will figure this out in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a01370dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T00:35:33.655339Z",
     "iopub.status.busy": "2023-02-08T00:35:33.654553Z",
     "iopub.status.idle": "2023-02-08T00:35:33.661295Z",
     "shell.execute_reply": "2023-02-08T00:35:33.660221Z"
    },
    "papermill": {
     "duration": 0.017075,
     "end_time": "2023-02-08T00:35:33.663704",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.646629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize weights and biases\n",
    "def init_params():\n",
    "    '''\n",
    "    Randomly generates the weights and biases for the first generation neural network.\n",
    "    '''\n",
    "\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75a095",
   "metadata": {
    "papermill": {
     "duration": 0.006256,
     "end_time": "2023-02-08T00:35:33.676824",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.670568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Initialization**\n",
    "Here we are pretty much initializing the weights and biases of the neural network. \n",
    "\n",
    "This is just a baseline for the neural network to use for the first training session, a.k.a. the first time we pass through the data through our network. Due to the randomization, it's expected that our neural network has around a 10% accuracy, since with these untouched, randomized weights and biases its like our neural network here is closing its eyes and completely guessing as to what the digit it is given is. \n",
    "\n",
    "As more training iterations occur, our network will begin \"learning\" and these weights and biases will be adjusted slowly to the weights and biases we want, which will slowly improve our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be62578f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T00:35:33.691595Z",
     "iopub.status.busy": "2023-02-08T00:35:33.691201Z",
     "iopub.status.idle": "2023-02-08T00:35:33.698273Z",
     "shell.execute_reply": "2023-02-08T00:35:33.697201Z"
    },
    "papermill": {
     "duration": 0.017352,
     "end_time": "2023-02-08T00:35:33.700746",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.683394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    '''\n",
    "    Pythonic ReLU function.\n",
    "    '''\n",
    "\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    '''\n",
    "    Pythonic softmax activation function.\n",
    "    '''\n",
    "\n",
    "    return np.exp(Z) / sum(np.exp(Z))\n",
    "\n",
    "# forward propagation\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    '''\n",
    "    Runs the data from the input nodes to the output nodes in a \n",
    "    forward motion.\n",
    "\n",
    "    X -- represents the input data / input layer\n",
    "    '''\n",
    "\n",
    "    z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(z1)\n",
    "    z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(z2)\n",
    "\n",
    "    return z1, A1, z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69432cc0",
   "metadata": {
    "papermill": {
     "duration": 0.007328,
     "end_time": "2023-02-08T00:35:33.714575",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.707247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Forward Propogation**\n",
    "So the first of the three sections I learned about neural networks. **Forward Propogation**. As of right now, the information I know about forward propogation is the data is being run through the neural network, a.k.a. some X (input) is run through the network, and the Y (output) is stored and, in the future, compared to the expected Y. \n",
    "\n",
    "Unfortunately, I don't understand all of the math behind the formulas used above, but I understand the *general* idea behind the formula as a whole!\n",
    "\n",
    "### Activation Functions\n",
    "The activation function used in this model is the ReLU function, which is seen in the below equations. What I learned activation functions do in neural networks is they determine how important a certain neuron is when processing the input. This function pretty much determines if a node should be activated or not, hence the name: *activation function*.\n",
    "\n",
    "Here are a few of the equations seen above (m $ \\rightarrow $ amount of training images):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddc631e",
   "metadata": {
    "papermill": {
     "duration": 0.006133,
     "end_time": "2023-02-08T00:35:33.727147",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.721014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$ A^{[0]} = X_{784 \\times m} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4979ad8",
   "metadata": {
    "papermill": {
     "duration": 0.006273,
     "end_time": "2023-02-08T00:35:33.739901",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.733628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$ z^{[1]}_{10 \\times m} = w^{[1]}_{10 \\times 784} * A^{[0]}_{784 \\times m} + b^{[1]}_{10 \\times m} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e93f46",
   "metadata": {
    "papermill": {
     "duration": 0.006114,
     "end_time": "2023-02-08T00:35:33.752455",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.746341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$ \\text{ReLU} \\rightarrow \\begin{bmatrix} x \\text{ if } x > 0 \\\\ 0 \\text{ if } x \\le 0 \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182b499",
   "metadata": {
    "papermill": {
     "duration": 0.006268,
     "end_time": "2023-02-08T00:35:33.765171",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.758903",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$ A^{[1]} = g(z^{[1]}) \\text{\\{g -> activation function\\}} = ReLU(z^{[1]}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b862f",
   "metadata": {
    "papermill": {
     "duration": 0.006153,
     "end_time": "2023-02-08T00:35:33.777767",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.771614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$ z^{[2]}_{10 \\times m} = w^{[2]}_{10 \\times 10} * A^{[1]}_{10 \\times m} + b^{[2]}_{10 \\times m} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a75d6",
   "metadata": {
    "papermill": {
     "duration": 0.006227,
     "end_time": "2023-02-08T00:35:33.790407",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.784180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$ \\text{softmax} \\rightarrow \\frac{e^{z_i}}{\\sum^K_{j=1}e^{z_j}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498ee89b",
   "metadata": {
    "papermill": {
     "duration": 0.006132,
     "end_time": "2023-02-08T00:35:33.803014",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.796882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$ A^{[2]} = \\text{softmax}(z^{[2]}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19134990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T00:35:33.818277Z",
     "iopub.status.busy": "2023-02-08T00:35:33.817364Z",
     "iopub.status.idle": "2023-02-08T00:35:33.828988Z",
     "shell.execute_reply": "2023-02-08T00:35:33.827772Z"
    },
    "papermill": {
     "duration": 0.021848,
     "end_time": "2023-02-08T00:35:33.831385",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.809537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    '''\n",
    "    Formats the expected value into an array of the correct size and format.\n",
    "    '''\n",
    "\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "\n",
    "    return one_hot_Y\n",
    "\n",
    "def deriv_ReLU(Z):\n",
    "    '''\n",
    "    Pythonic derivative of ReLU function.\n",
    "    '''\n",
    "\n",
    "    return Z > 0\n",
    "\n",
    "# backward propagation\n",
    "def back_prop(z1, A1, z2, A2, W1, W2, X, Y):\n",
    "    '''\n",
    "    Runs the data from the output nodes to the input nodes and\n",
    "    calculates the error in a backward motion.\n",
    "    '''\n",
    "\n",
    "    m = Y.size\n",
    "    one_hot_Y = one_hot(Y)\n",
    "\n",
    "    dz2 = 2 * (A2 - one_hot_Y)\n",
    "    dW2 = 1 / m * dz2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dz2)\n",
    "    dz1 = W2.T.dot(dz2) * deriv_ReLU(z1)\n",
    "    dW1 = 1 / m * dz1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dz1)\n",
    "\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f739ec9c",
   "metadata": {
    "papermill": {
     "duration": 0.006772,
     "end_time": "2023-02-08T00:35:33.845433",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.838661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Back Propogation**\n",
    "Second of the three seconds is **back propagation**! This was the area where I learned the most about neural networks, as I already had a general idea that the way they trained was by feeding them input data and evaluating the output. However, I was uncertain of the process of evaluating said input, which I learned during my learning process with back propagation!\n",
    "\n",
    "I understand basic ideas and mathematics behind back propagation, but I am also lacking when it comes to specifics and specific math behind the idea of back propagation. \n",
    "\n",
    "What I generally understand is these functions are trying to find the amount of error behind the output recieved from *forward propagation* and evaluate what weights and biases contributed the most and the amount of impact they had on the error, hence why we are finding \"derivatives\" of the weights and biases, as they represent the amount of an impact they had on the error within the output.\n",
    "\n",
    "The weights and biases are then adjusted based on these derivatives, however we aren't on that stage yet. I'm a little loose on the math behind using the derivative of our activation function (ReLU), however the other function: *one_hot* I understand. We are isolating the output and formatting it into a $ 1 \\times 10 $ array so we can take the difference of the expected output and our output *A2* that we got from our forward propagation functions.\n",
    "\n",
    "I still have a lot to learn in this area, however, I believe that I learned a lot during my research and hands-on project! Here are some of the formulas I used (again; m $ \\rightarrow $ amount of training images):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061157b0",
   "metadata": {
    "papermill": {
     "duration": 0.006316,
     "end_time": "2023-02-08T00:35:33.858356",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.852040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$ dz^{[2]}_{10 \\times m} = 2 (A^{[2]}_{10 \\times m} - Y_{10 \\times m}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6cd15b",
   "metadata": {
    "papermill": {
     "duration": 0.006211,
     "end_time": "2023-02-08T00:35:33.871010",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.864799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$ dw^{[2]}_{10 \\times 10} = \\frac1m dz^{[2]}_{10 \\times m} A^{[1]T}_{m \\times 10} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1ff80b",
   "metadata": {
    "papermill": {
     "duration": 0.006284,
     "end_time": "2023-02-08T00:35:33.883746",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.877462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$ db^{[2]}_{10 \\times 1} = \\frac1m \\sum dz^{[2]} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e473ab4",
   "metadata": {
    "papermill": {
     "duration": 0.006217,
     "end_time": "2023-02-08T00:35:33.896441",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.890224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$ \\text{ReLU'} \\rightarrow \\begin{bmatrix} 1 \\text{ if } x > 0 \\\\ 0 \\text{ if } x \\le 0 \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e045f3",
   "metadata": {
    "papermill": {
     "duration": 0.006319,
     "end_time": "2023-02-08T00:35:33.909351",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.903032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$ dz^{[1]}_{10 \\times m} = w^{[2]T}_{10 \\times 10} dz^{[2]}_{10 \\times m} * g'(z^{[1]})_{10 \\times m} = w^{[2]T}_{10 \\times 10} dz^{[2]}_{10 \\times m} * ReLU'(z^{[1]})_{10 \\times m} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c1d3d9",
   "metadata": {
    "papermill": {
     "duration": 0.006242,
     "end_time": "2023-02-08T00:35:33.922131",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.915889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$ dw^{[1]}_{10 \\times 784} = \\frac1m dz^{[1]}_{10 \\times m} X^T_{m \\times 784} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce827fb",
   "metadata": {
    "papermill": {
     "duration": 0.006379,
     "end_time": "2023-02-08T00:35:33.935087",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.928708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "$ db^{[1]}_{10 \\times 1} = \\frac1m \\sum dz^{[1]}_{10 \\times 1} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "976cd439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T00:35:33.950683Z",
     "iopub.status.busy": "2023-02-08T00:35:33.949857Z",
     "iopub.status.idle": "2023-02-08T00:35:33.955856Z",
     "shell.execute_reply": "2023-02-08T00:35:33.955074Z"
    },
    "papermill": {
     "duration": 0.016437,
     "end_time": "2023-02-08T00:35:33.958136",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.941699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    predictions = get_predictions(A2)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c41148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T00:35:33.973682Z",
     "iopub.status.busy": "2023-02-08T00:35:33.973243Z",
     "iopub.status.idle": "2023-02-08T00:35:33.982542Z",
     "shell.execute_reply": "2023-02-08T00:35:33.981436Z"
    },
    "papermill": {
     "duration": 0.020039,
     "end_time": "2023-02-08T00:35:33.985042",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.965003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update parameters\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    '''\n",
    "    Adjusts the weights and biases after front and back propagation is\n",
    "    completed.\n",
    "    '''\n",
    "\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# gradient descent\n",
    "def gradient_descent(X, Y, iterations, alpha):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        z1, A1, z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = back_prop(z1, A1, z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 50 == 0:\n",
    "            print('Iteration: ', i)\n",
    "            print('Accuracy: ', get_accuracy(get_predictions(A2), Y))\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb8038",
   "metadata": {
    "papermill": {
     "duration": 0.006704,
     "end_time": "2023-02-08T00:35:33.998473",
     "exception": false,
     "start_time": "2023-02-08T00:35:33.991769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Updating Parameters & Gradient Descent**\n",
    "This is the final phase of the neural network training process that I learned about and it really wrapped everything up in a nice shiny bow for me, showing the completely loop that the training of a neural network really is. \n",
    "\n",
    "The input data is converted into an observed output, this output is compared to the expected output, and this error is then used to adjust the weights and biases accordingly. Then we go through this loop over and over again! Although this may sound repeatative or boring to some, the learning process of this whole system was truly interesting to me and I'm excited to learn more! \n",
    "\n",
    "Anyways, going back on track, after we figure out what weights and biases had what effects on the error, we have to adjust the weights and biases accordingly so they do better the next time we run the input data through! We do this by updating the parameters using the function above. \n",
    "\n",
    "### Learning Rate\n",
    "A new term introduced was a *learning rate* which is represented by the variable alpha above. The optimal (or just randomly chosen) value used in this model was 1.5, and this pretty much determines how much will the neural network adjust itself when an error is found.\n",
    "\n",
    "Too much of a learning rate may cause occillation in the weights and biases by adjusting the weights and biases too much and being too sensitive to small errors, causing little to no learning within our neural network. The opposite could happen when the learning rate is too small, not allowing our neural network to make major enough changes to error when an error presents itself. This is why finding a sweet spot with learning rate is important.\n",
    "\n",
    "The function as a whole runs the input data through the loop mentioned earlier, and constantly updates the parameters over and over until the desired amount of iterations are reached. The function then returns the weights and biases to be used by our neural network for testing or production!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a778798",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T00:35:34.014157Z",
     "iopub.status.busy": "2023-02-08T00:35:34.013712Z",
     "iopub.status.idle": "2023-02-08T00:35:34.020947Z",
     "shell.execute_reply": "2023-02-08T00:35:34.019795Z"
    },
    "papermill": {
     "duration": 0.018062,
     "end_time": "2023-02-08T00:35:34.023389",
     "exception": false,
     "start_time": "2023-02-08T00:35:34.005327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def submit(W1, b1, W2, b2):\n",
    "    submission_data = []\n",
    "    \n",
    "    for i in range(len(X_test[0])):\n",
    "        current_image = X_test[:, i, None]\n",
    "        prediction = make_predictions(X_test[:, i, None], W1, b1, W2, b2)\n",
    "        submission_data.append([i + 1, prediction[0]])\n",
    "        \n",
    "    submission = pd.DataFrame(submission_data, columns=['ImageId', 'Label'])\n",
    "    submission.to_csv('/kaggle/working/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e594d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T00:35:34.038587Z",
     "iopub.status.busy": "2023-02-08T00:35:34.038151Z",
     "iopub.status.idle": "2023-02-08T00:36:50.911546Z",
     "shell.execute_reply": "2023-02-08T00:36:50.910120Z"
    },
    "papermill": {
     "duration": 76.884392,
     "end_time": "2023-02-08T00:36:50.914609",
     "exception": false,
     "start_time": "2023-02-08T00:35:34.030217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Accuracy:  0.10238095238095238\n",
      "Iteration:  50\n",
      "Accuracy:  0.5036190476190476\n",
      "Iteration:  100\n",
      "Accuracy:  0.7549523809523809\n",
      "Iteration:  150\n",
      "Accuracy:  0.8137142857142857\n",
      "Iteration:  200\n",
      "Accuracy:  0.8378571428571429\n",
      "Iteration:  250\n",
      "Accuracy:  0.8528571428571429\n",
      "Iteration:  300\n",
      "Accuracy:  0.860904761904762\n",
      "Iteration:  350\n",
      "Accuracy:  0.8677619047619047\n",
      "Iteration:  400\n",
      "Accuracy:  0.8734047619047619\n",
      "Iteration:  450\n",
      "Accuracy:  0.8775\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 500, 0.15)\n",
    "    submit(W1, b1, W2, b2)\n",
    "    \n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd8c6e2",
   "metadata": {
    "papermill": {
     "duration": 0.00772,
     "end_time": "2023-02-08T00:36:50.930513",
     "exception": false,
     "start_time": "2023-02-08T00:36:50.922793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Conclusion**\n",
    "Overall this was a fun learning experience and I learned a lot! I'm not sure if this is the usual way of using Jupiter but it was cool making a journal like this to log my thoughts throughout the whole process.\n",
    "\n",
    "Thank you so much for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 93.675018,
   "end_time": "2023-02-08T00:36:51.662233",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-08T00:35:17.987215",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
